"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1986],{31542:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var t=i(85893),a=i(11151);const o={slug:"kafka-connect-jdbc-sink-iam-auth",title:"Kafka Connect - JDBC Sink",authors:"javier",tags:["Kafka-Connect","JDBC-Sink","Kafka","IAM-Auth","MSK","K8s"]},s="From Kafka to an RDS",r={permalink:"/blog/kafka-connect-jdbc-sink-iam-auth",editUrl:"https://github.com/JavierMonton/blog/edit/main/website/blog/2023-12-16-kafka-connect-jdbc-sink/index.md",source:"@site/blog/2023-12-16-kafka-connect-jdbc-sink/index.md",title:"Kafka Connect - JDBC Sink",description:"A guide to move data from Kafka to an AWS RDS using Kafka Connect and the JDBC Sink Connector with IAM Auth.",date:"2023-12-16T00:00:00.000Z",formattedDate:"December 16, 2023",tags:[{label:"Kafka-Connect",permalink:"/blog/tags/kafka-connect"},{label:"JDBC-Sink",permalink:"/blog/tags/jdbc-sink"},{label:"Kafka",permalink:"/blog/tags/kafka"},{label:"IAM-Auth",permalink:"/blog/tags/iam-auth"},{label:"MSK",permalink:"/blog/tags/msk"},{label:"K8s",permalink:"/blog/tags/k-8-s"}],readingTime:19.33,hasTruncateMarker:!1,authors:[{name:"Javier Mont\xf3n",title:"Software Engineer",url:"https://github.com/JavierMonton",imageURL:"https://github.com/JavierMonton.png",key:"javier"}],frontMatter:{slug:"kafka-connect-jdbc-sink-iam-auth",title:"Kafka Connect - JDBC Sink",authors:"javier",tags:["Kafka-Connect","JDBC-Sink","Kafka","IAM-Auth","MSK","K8s"]},unlisted:!1,prevItem:{title:"Kafka Connect, MM2, and Offset Management",permalink:"/blog/kafka-connect-mm2-offset-management"},nextItem:{title:"Big Data Types Library",permalink:"/blog/big-data-types-library"}},l={authorsImageUrls:[void 0]},c=[{value:"Kafka Connect",id:"kafka-connect",level:2},{value:"Single and distributed modes",id:"single-and-distributed-modes",level:3},{value:"Deploying in K8s",id:"deploying-in-k8s",level:3},{value:"Using MSK (Kafka)",id:"using-msk-kafka",level:3},{value:"Environment variables",id:"environment-variables",level:4},{value:"Formatting logs as JSON",id:"formatting-logs-as-json",level:3},{value:"Using JSONEventLayoutV1",id:"using-jsoneventlayoutv1",level:4},{value:"Properties files",id:"properties-files",level:4},{value:"Adding plugins",id:"adding-plugins",level:3},{value:"Adding libraries",id:"adding-libraries",level:3},{value:"Kafka Connect REST API",id:"kafka-connect-rest-api",level:3},{value:"Securing the API",id:"securing-the-api",level:4},{value:"JAX-RS Security Extensions",id:"jax-rs-security-extensions",level:5},{value:"JDBC Sink Connector",id:"jdbc-sink-connector",level:2},{value:"Drivers",id:"drivers",level:3},{value:"IAM Auth",id:"iam-auth",level:3},{value:"META-INF/services and multiple drivers",id:"meta-infservices-and-multiple-drivers",level:4},{value:"Topic &amp; Destination table",id:"topic--destination-table",level:3},{value:"One topic / table per connector",id:"one-topic--table-per-connector",level:4},{value:"Using patterns for topics and table names",id:"using-patterns-for-topics-and-table-names",level:3},{value:"Case insensitive",id:"case-insensitive",level:3},{value:"Deploy new connectors in K8s",id:"deploy-new-connectors-in-k8s",level:2},{value:"connect-operator",id:"connect-operator",level:2},{value:"Configuring the connect-operator",id:"configuring-the-connect-operator",level:3},{value:"Listening to config maps",id:"listening-to-config-maps",level:3},{value:"Config Map",id:"config-map",level:3},{value:"Replacing variables",id:"replacing-variables",level:3},{value:"RBAC permissions to read config maps",id:"rbac-permissions-to-read-config-maps",level:3},{value:"Custom Kafka groups for your connectors",id:"custom-kafka-groups-for-your-connectors",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",h5:"h5",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"A guide to move data from Kafka to an AWS RDS using Kafka Connect and the JDBC Sink Connector with IAM Auth."}),"\n",(0,t.jsx)(n.h2,{id:"kafka-connect",children:"Kafka Connect"}),"\n",(0,t.jsx)(n.p,{children:"For these examples, we are using Confluent's Kafka Connect on its Docker version, as we are going to deploy it in a Kubernetes cluster."}),"\n",(0,t.jsx)(n.h3,{id:"single-and-distributed-modes",children:"Single and distributed modes"}),"\n",(0,t.jsx)(n.p,{children:"Kafka Connect comes with two modes of execution, single and distributed. The main difference between them is that the single mode runs all the connectors in the same JVM, while the distributed mode runs each connector in its own JVM. The distributed mode is the recommended one for production environments, as it provides better scalability and fault tolerance.\nIn the case of K8s, it means we will be using more than one pod to run Kafka Connect."}),"\n",(0,t.jsxs)(n.admonition,{type:"warning",children:[(0,t.jsx)(n.p,{children:"Be aware that these two modes use different class paths, so if you are doing changes inside the docker and you are running the single mode locally but distributed in production, you might have different results.\nI strongly recommend to check manually which are the class paths in each case using something like"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ps aux | grep java\n"})}),(0,t.jsx)(n.p,{children:"And  you will get something like this:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"java -Xms256M -Xmx2G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true -Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/var/log/kafka -Dlog4j.configuration=file:/etc/kafka/connect-log4j.properties -cp /etc/kafka-connect/jars/*:/usr/share/java/kafka/*:/usr/share/java/confluent-common/*:/usr/share/java/kafka-serde-tools/*:/usr/share/java/monitoring-interceptors/*:/usr/bin/../share/java/kafka/*:/usr/bin/../share/java/confluent-telemetry/* org.apache.kafka.connect.cli.ConnectDistributed /etc/kafka-connect/kafka-connect.properties\n"})}),(0,t.jsxs)(n.p,{children:["And you'll find all the directories (after ",(0,t.jsx)(n.code,{children:"-cp"}),") included in the running Kafka Connect."]}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Note that a folder called ",(0,t.jsx)(n.code,{children:"cp-base-new"})," is widely used in the Single mode, but is not very well documented."]}),"\n",(0,t.jsx)(n.li,{children:"Setting your deployment to 1 replicas will run Kafka Connect in Single mode while setting it to 2 or more will run it in Distributed mode."}),"\n"]})]}),"\n",(0,t.jsx)(n.h3,{id:"deploying-in-k8s",children:"Deploying in K8s"}),"\n",(0,t.jsxs)(n.p,{children:["This should be fairly straightforward, as we are using Confluent's Kafka Connect Docker image, which is already prepared to be deployed in K8s.\nConfluent provides a ",(0,t.jsx)(n.a,{href:"https://github.com/confluentinc/cp-helm-charts/blob/master/charts/cp-kafka-connect/README.md",children:"Helm chart"})," as an example, so it should be easy. You can also create your own."]}),"\n",(0,t.jsx)(n.h3,{id:"using-msk-kafka",children:"Using MSK (Kafka)"}),"\n",(0,t.jsx)(n.p,{children:"If you are using the AWS's Kafka version, MSK, and you are authenticating using IAM, you will need to do a few things:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Configure some environment variables in Kafka Connect"}),"\n",(0,t.jsx)(n.li,{children:"Add the required AWS libraries to the classpath"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"environment-variables",children:"Environment variables"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"CONNECT_BOOTSTRAP_SERVERS"})," will have the brokers, as usual, but using the ",(0,t.jsx)(n.code,{children:"9098"})," port."]}),"\n",(0,t.jsx)(n.p,{children:"You need to specify the IAM callback handler as well as SASL:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"CONNECT_SASL_CLIENT_CALLBACK_HANDLER_CLASS = software.amazon.msk.auth.iam.IAMClientCallbackHandler\nCONNECT_SASL_MECHANISM = AWS_MSK_IAM\nCONNECT_SECURITY_PROTOCOL = SASL_SSL\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Also, you have to provide a ",(0,t.jsx)(n.code,{children:"JAAS"})," file with the credentials. You can find more info about this in the ",(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/msk/latest/developerguide/msk-password.html#msk-password-sasl-plain",children:"AWS's documentation"}),".\nFor IAM, something like this should work:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'CONNECT_SASL_JAAS_CONFIG = \n      software.amazon.msk.auth.iam.IAMLoginModule required\n      awsRoleArn="arn:aws:iam::{account}:role/{role}"\n      awsStsRegion="{region}";\n'})}),"\n",(0,t.jsx)(n.p,{children:"If you do this in yaml for Helm, it will look like this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'  - name: CONNECT_SASL_JAAS_CONFIG\n    value: >-\n      software.amazon.msk.auth.iam.IAMLoginModule required\n      awsRoleArn="arn:aws:iam::{account}:role/{role}"\n      awsStsRegion="{region}";\n'})}),"\n",(0,t.jsx)(n.p,{children:"When Kafka Connect creates a new connector, it will use its own credentials configuration, so if you want to have the same IAM auth, you will need to add the same values to these environment variables:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"CONNECT_CONSUMER_SASL_CLIENT_CALLBACK_HANDLER_CLASS"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"CONNECT_CONSUMER_SASL_MECHANISM"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"CONNECT_CONSUMER_SECURITY_PROTOCOL"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"CONNECT_CONSUMER_SASL_JAAS_CONFIG"})}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"If you are using your own Helm template, you could create some variables for these values, so you can reuse them in the different environment variables, to avoid writing them twice."})}),"\n",(0,t.jsx)(n.h3,{id:"formatting-logs-as-json",children:"Formatting logs as JSON"}),"\n",(0,t.jsx)(n.p,{children:"Logs are very important, and having a good format is key to being able to read and process them easily. Usually, in production, we could want to have them as JSON, and Kafka Connect does not make it as easy for us as we might expect."}),"\n",(0,t.jsxs)(n.p,{children:["If you only want to change the log level or format your logs a bit, you could use the environment variables available for that, they are ",(0,t.jsx)(n.a,{href:"https://docs.confluent.io/platform/current/connect/logging.html#use-environment-variables-docker",children:"described in their docs"}),"\nbut if you want to proper format all logs as JSON, you will need to do a few more things."]}),"\n",(0,t.jsx)(n.h4,{id:"using-jsoneventlayoutv1",children:"Using JSONEventLayoutV1"}),"\n",(0,t.jsxs)(n.p,{children:["Kafka Connect uses ",(0,t.jsx)(n.code,{children:"log4j1"})," (not ",(0,t.jsx)(n.code,{children:"log4j2"}),"), so we will need to use a ",(0,t.jsx)(n.code,{children:"log4j.properties"})," file to configure it. They are using a patched version of the original Log4j1 that is supposed to fix some vulnerabilities."]}),"\n",(0,t.jsxs)(n.p,{children:["We can use a dependency that automatically converts all of our logs into JSON, like ",(0,t.jsx)(n.a,{href:"https://github.com/logstash/log4j-jsonevent-layout",children:"log4j-jsonevent-layout"}),". See ",(0,t.jsx)(n.a,{href:"#adding-libraries",children:"Adding libraries"})," for more info about how to add new libraries.\nIf we have this library in the classpath, we can now use the ",(0,t.jsx)(n.code,{children:"JSONEventLayoutV1"})," in our ",(0,t.jsx)(n.code,{children:"log4j.properties"})," file. Like:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-properties",children:"log4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout=net.logstash.log4j.JSONEventLayoutV1\n"})}),"\n",(0,t.jsx)(n.h4,{id:"properties-files",children:"Properties files"}),"\n",(0,t.jsxs)(n.p,{children:["Confluent will tell you that you can modify the template for logs in ",(0,t.jsx)(n.code,{children:"/etc/confluent/docker/log4j.properties.template"}),", but you might need some extra steps if you want ",(0,t.jsx)(n.strong,{children:"all"})," logs as JSON."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Template for most of the logs, as described, in ",(0,t.jsx)(n.code,{children:"/etc/confluent/docker/log4j.properties.template"})]}),"\n",(0,t.jsxs)(n.li,{children:['Logs from the "Admin Client" in ',(0,t.jsx)(n.code,{children:"/etc/kafka/log4j.properties"})]}),"\n",(0,t.jsxs)(n.li,{children:["Some tool logs in ",(0,t.jsx)(n.code,{children:"/etc/confluent/docker/tools-log4j.properties.template"})]}),"\n",(0,t.jsxs)(n.li,{children:["Some startup logs in ",(0,t.jsx)(n.code,{children:"/etc/kafka-connect/log4j.properties"})]}),"\n",(0,t.jsxs)(n.li,{children:["There are also some random logs not using Log4j, they are defined in ",(0,t.jsx)(n.code,{children:"/usr/lib/jvm/jre/conf/logging.properties"})]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"If you want to format everything to JSON, I would recommend entering inside the docker image, looking for those files, and changing them as desired. Your Dockerfile could then replace them while creating the image."}),"\n",(0,t.jsxs)(n.admonition,{type:"warning",children:[(0,t.jsxs)(n.p,{children:["There are still some logs during the start-up not formatted as JSON. Confluent's Kafka Connect is using a ",(0,t.jsx)(n.a,{href:"https://github.com/confluentinc/confluent-docker-utils/blob/master/confluent/docker_utils/cub.py",children:"Python script to start up the service"}),",\nand that service is using some ",(0,t.jsx)(n.code,{children:"prints"})," that do not belong to any Log4j, so they are not formatted in any way."]}),(0,t.jsxs)(n.p,{children:["If you want to format those ",(0,t.jsx)(n.code,{children:"prints"})," too, you will need to do something else as they don't have any configuration files. You could use a ",(0,t.jsx)(n.code,{children:"sed"})," command to replace them, or you could modify the ",(0,t.jsx)(n.code,{children:"cub.py"})," file in your image with the desired format."]})]}),"\n",(0,t.jsx)(n.h3,{id:"adding-plugins",children:"Adding plugins"}),"\n",(0,t.jsx)(n.p,{children:"Adding plugins should be straightforward, the documentation explains pretty well how to do it. Note that you can add plugins inside the plugins folder or you could modify the plugins folder with"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"plugin.path=/usr/local/share/kafka/plugins\n"})}),"\n",(0,t.jsx)(n.p,{children:"In any case, copying and pasting files into a Docker can limit a bit the flexibility of the solution, so I would recommend building a project where you can add all the dependencies that you need, meaning that libraries and plugins can be built and copied inside the Docker during the CI.\nBy doing this, you will be able to use Gradle, Maven, SBT, or any other building tool to manage your dependencies, upgrade versions, and build plugins."}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"Note that Plugins and libraries are not included in the same path, so I would recommend building a different project for each.\nFor example, we could build a main project that can build the Kafka Connect image with their libraries and a subproject that can build plugins in a different folder. Then, the Dockerfile could easily copy both folders into the image in the right paths."})}),"\n",(0,t.jsx)(n.p,{children:"If you build a project like that, to add the JDBC plugin, for example, in Gradle you only need to add this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-kotlin",children:'dependencies {\n    implementation("io.confluent:kafka-connect-jdbc:10.7.4")\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"adding-libraries",children:"Adding libraries"}),"\n",(0,t.jsx)(n.p,{children:"As mentioned earlier, libraries must go in the classpath, not in the plugins' folder.\nIf you are using a project to build your libraries and plugins, you could use many different plugins to pack all the dependencies into a .jar that can be copied into the Docker image."}),"\n",(0,t.jsx)(n.p,{children:"For example, with Gradle, we could include the AWS library needed for IAM authentication, and the Log4j JSON formatter, like this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-kotlin",children:'dependencies {\n    implementation("software.amazon.msk:aws-msk-iam-auth:1.1.7")\n    implementation("net.logstash.log4j:jsonevent-layout:1.7")\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:"Using a plugin to build a fat jar, everything should be included in one .jar file that we can copy into the Docker image."}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"For the JDBC Sink, we will need to also include a Driver and more libraries in case we want to use IAM Auth with RDS, we will see that later."})}),"\n",(0,t.jsx)(n.h3,{id:"kafka-connect-rest-api",children:"Kafka Connect REST API"}),"\n",(0,t.jsxs)(n.p,{children:["By default, Kafka Connect exposes its REST API in the port ",(0,t.jsx)(n.code,{children:"8083"}),". You can find more info about the API in the ",(0,t.jsx)(n.a,{href:"https://docs.confluent.io/platform/current/connect/references/restapi.html",children:"official documentation"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["If you want to control who can access the API or change its port, you can use the ",(0,t.jsx)(n.code,{children:"CONNECT_LISTENERS"})," and/or ",(0,t.jsx)(n.code,{children:"CONNECT_REST_ADVERTISED_PORT"})," environment variables.\nFor example, if you want to change the port to ",(0,t.jsx)(n.code,{children:"8084"}),", you could do this:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'  - name: CONNECT_REST_ADVERTISED_PORT\n    value: "8084"\n'})}),"\n",(0,t.jsx)(n.p,{children:"Also, you can even open the API in multiple ports, by doing this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'  - name: CONNECT_LISTENERS\n    value: "http://0.0.0.0:8084,http://0.0.0.0:8085"\n  - name: CONNECT_REST_ADVERTISED_PORT\n    value: "8084"\n'})}),"\n",(0,t.jsx)(n.h4,{id:"securing-the-api",children:"Securing the API"}),"\n",(0,t.jsxs)(n.p,{children:["Kafka Connect's REST API lacks security options, it only allows you to use a basic authentication, which might not be what you are looking for. Also, the code seems to have several places where they do an ",(0,t.jsx)(n.code,{children:"if - else"})," to check if basic auth is enabled or not."]}),"\n",(0,t.jsx)(n.p,{children:"But, there is also another way we can use to build our own security layer."}),"\n",(0,t.jsx)(n.h5,{id:"jax-rs-security-extensions",children:"JAX-RS Security Extensions"}),"\n",(0,t.jsxs)(n.p,{children:["Without entering too much into details, Kafka Connect, as well as Schema Registry, are using ",(0,t.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Jakarta_RESTful_Web_Services",children:"JAX-RS"})," to build their REST APIs, and JAX-RS allows us to add our own security extensions.\nFollowing this pattern, we could add a simple filter to check if the user is authenticated or not, and if not, we could return a ",(0,t.jsx)(n.code,{children:"401"})," error."]}),"\n",(0,t.jsxs)(n.p,{children:["About how to authenticate a user, we could use different methods, depending on our setup. For example, we could use AWS IAM API to check if a user has permissions or not, or as we are deploying this in Kubernetes, we could rely on ",(0,t.jsx)(n.a,{href:"https://learnk8s.io/microservices-authentication-kubernetes",children:"Kubernetes identities"})," which will allow us to authenticate pods using a JWT token."]}),"\n",(0,t.jsxs)(n.p,{children:["To do this, you have to create a JAX-RS plugin and then register it in Kafka Connect. Once your plugin is ready, you can register it by extending ",(0,t.jsx)(n.code,{children:"ConnectRestExtension"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-scala",children:"class MySecurityExtension extends ConnectRestExtension {}\n"})}),"\n",(0,t.jsx)(n.p,{children:"This class will need to be packed with your libraries and included in the classpath, as we did with other libraries."}),"\n",(0,t.jsx)(n.h2,{id:"jdbc-sink-connector",children:"JDBC Sink Connector"}),"\n",(0,t.jsxs)(n.p,{children:["We need to download the plugin and add it to the plugins' folder. By default, it's ",(0,t.jsx)(n.code,{children:"/usr/share/confluent-hub-components/"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["You can get the .jar with ",(0,t.jsx)(n.code,{children:"wget"})," and copy it inside the Docker image, in the aforementioned folder. Or, as suggested earlier, if you are building a project using a building tool, like Gradle, you can use Maven to download all the plugins you might need. We only need to add the dependency:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-kotlin",children:'dependencies {\n    implementation("io.confluent:kafka-connect-jdbc:10.7.4")\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:"And build the .jar. Then, we can copy it inside the Docker image."}),"\n",(0,t.jsx)(n.h3,{id:"drivers",children:"Drivers"}),"\n",(0,t.jsx)(n.p,{children:"Only the plugin is not enough to connect to a database, we will also need the driver. In our case, we are using PostgreSQL RDS, so we will need the driver for Postgres."}),"\n",(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsxs)(n.p,{children:["Several drivers are already included in the Kafka Connect image, but they are not inside the default classpath, so if we try to run the connector without adding the driver properly, we will get an error like ",(0,t.jsx)(n.code,{children:"No suitable driver found"}),".\nThey are placed in ",(0,t.jsx)(n.code,{children:"/usr/share/confluent-hub-components/"}),", but as we can see using something like ",(0,t.jsx)(n.code,{children:"ps aux | grep java"}),", they are not included in the classpath. So, we have three options:"]}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Move the driver to the classpath"}),"\n",(0,t.jsx)(n.li,{children:"Add the drivers' folder to the classpath"}),"\n",(0,t.jsx)(n.li,{children:"Find our own driver and copy it inside the Docker image, in the classpath"}),"\n"]})]}),"\n",(0,t.jsx)(n.p,{children:"I would go for the third option, which gives us more flexibility about which version of the driver we want to use."}),"\n",(0,t.jsx)(n.p,{children:"So, we can download the driver and pack it with our libraries, and then copy it inside the Docker image:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-kotlin",children:'implementation("org.postgresql:postgresql:42.7.1")\n'})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["Note that the JDBC Sink has to be placed in ",(0,t.jsx)(n.code,{children:"plugins"})," folder, while the driver has to be placed in the ",(0,t.jsx)(n.code,{children:"library"})," classpath."]})}),"\n",(0,t.jsx)(n.h3,{id:"iam-auth",children:"IAM Auth"}),"\n",(0,t.jsxs)(n.p,{children:["If you are using IAM Auth with RDS, you will need to add some extra libraries to the classpath. You can find more info about this in the ",(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.Connecting.Java.html",children:"AWS's documentation"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["The bad news is that a simple driver can not use IAM Auth, if you try to connect to the database using IAM Auth, you will get an error like ",(0,t.jsx)(n.code,{children:"The server requested password-based authentication, but no password was provided."}),". You would need to create a token manually and pass it through the connection."]}),"\n",(0,t.jsxs)(n.p,{children:["The good news is that there is a library created by AWS that acts as a ",(0,t.jsx)(n.a,{href:"https://github.com/awslabs/aws-advanced-jdbc-wrapper",children:"wrapper for your JDBC Drivers"}),", adding extra features, including IAM Auth."]}),"\n",(0,t.jsx)(n.p,{children:"To use IAM Auth, we only need to add this driver to the classpath, and change a bit our JDBC URL."}),"\n",(0,t.jsx)(n.p,{children:"Add the dependency (also needed libraries for AWS RDS):"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-kotlin",children:'implementation("com.github.awslabs:aws-advanced-jdbc-wrapper:2.3.2")\nimplementation("software.amazon.awssdk:rds:2.20.145")\n'})}),"\n",(0,t.jsx)(n.p,{children:"You can follow their docs, but in our case, we will need to change the JDBC URL to add a couple of things:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"URL using the new driver"}),"\n",(0,t.jsx)(n.li,{children:"IAM Auth enabled flag enabled"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"jdbc:aws-wrapper:postgresql://{host}:{port}/postgres?wrapperPlugins=iam\n"})}),"\n",(0,t.jsxs)(n.admonition,{type:"tip",children:[(0,t.jsx)(n.p,{children:"Some tips:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"This JDBC URL goes inside the connector's configuration"}),"\n",(0,t.jsx)(n.li,{children:"A username is still required, and it should be the same as the role used to connect to the database"}),"\n",(0,t.jsxs)(n.li,{children:["You can also help the wrapper to find the dialect used, by adding ",(0,t.jsx)(n.code,{children:"&wrapperDialect=rds-pg"})]}),"\n",(0,t.jsxs)(n.li,{children:["You can also help Kafka Connect to find the dialect used, by adding another property in your connector's configuration: ",(0,t.jsx)(n.code,{children:'dialectName: "PostgreSqlDatabaseDialect"'})]}),"\n"]})]}),"\n",(0,t.jsx)(n.h4,{id:"meta-infservices-and-multiple-drivers",children:"META-INF/services and multiple drivers"}),"\n",(0,t.jsxs)(n.p,{children:["At this point, we are including the JDBC PostgreSQL driver and the wrapper in the classpath, both are JDBC Drivers, if we are including different .jar files, everything should be fine,\nbut if we are building a fat-jar, we might have some issues. Each one of these drivers is creating a file called ",(0,t.jsx)(n.code,{children:"META-INF/services/java.sql.Driver"}),",\nand they are including the name of the driver in it. If our fat-jar is not merging them to include both classes, we will get an error like ",(0,t.jsx)(n.code,{children:"No suitable driver found"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"Depending on the building tool and the plugin used, we might need to add some extra configuration to merge these files. For example, in Gradle we could need to add something like this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"mergeServiceFiles()\n"})}),"\n",(0,t.jsx)(n.p,{children:"Or in the SBT Assembly plugin, we could need to add something like:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sbt",children:"assembly / assemblyMergeStrategy := MergeStrategy.concat\n"})}),"\n",(0,t.jsx)(n.h3,{id:"topic--destination-table",children:"Topic & Destination table"}),"\n",(0,t.jsx)(n.p,{children:"The JDBC Sink Connector allows us to decide which topics and tables we want to use, and we have two ways of doing it:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"One topic / table per connector. In this case, we can directly write the topic and table names in the connector's configuration."}),"\n",(0,t.jsx)(n.li,{children:"Multiple topics / tables per connector. In this case, we will need to use a pattern for topics and another for tables."}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"one-topic--table-per-connector",children:"One topic / table per connector"}),"\n",(0,t.jsx)(n.p,{children:"This is the easiest way, we only need to add the topic and table names in the connector's configuration, like this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'topics: "my.first.topic"\ntable.name.format: "first_table"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"using-patterns-for-topics-and-table-names",children:"Using patterns for topics and table names"}),"\n",(0,t.jsxs)(n.p,{children:["The JDBC does some magic to map topics to tables, but it's not always what we want. For example, if we have a topic called ",(0,t.jsx)(n.code,{children:"my.topic"})," it will take ",(0,t.jsx)(n.code,{children:"my"})," as schema name and ",(0,t.jsx)(n.code,{children:"topic"})," as table name. More details about ",(0,t.jsx)(n.a,{href:"https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/overview.html#table-parsing",children:"table parsing"})," can be found in their docs."]}),"\n",(0,t.jsxs)(n.p,{children:["But, we likely use a pattern for our topics, especially if we are building a Datalake, so we might want to create tables based on a different pattern. For example, we could have a topic called ",(0,t.jsx)(n.code,{children:"my.first.topic"})," and we want to create a table called ",(0,t.jsx)(n.code,{children:"first_table"})," in our database. This can still be achieved using a ",(0,t.jsx)(n.code,{children:"router"})," and a ",(0,t.jsx)(n.code,{children:"table.name.format"})," property."]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["Be aware that not all types accepted in your Kafka topics are accepted in your database, JDBC Driver and/or the JDBC Sink. For example, the list of valid types from the perspective of the ",(0,t.jsx)(n.a,{href:"https://github.com/confluentinc/kafka-connect-jdbc/blob/master/src/main/java/io/confluent/connect/jdbc/dialect/PostgreSqlDatabaseDialect.java#L299",children:"JDBC Sink is here"}),"."]})}),"\n",(0,t.jsx)(n.h3,{id:"case-insensitive",children:"Case insensitive"}),"\n",(0,t.jsxs)(n.p,{children:["By default, the JDBC Sink will use quotes for all table and column names, which is usually fine, but PostgreSQL is case insensitive if quotes are not used, so if your data from Kafka comes with uppercase letters, for example, if you are using ",(0,t.jsx)(n.code,{children:"camelCase"}),", but if you are not using quotes in your database, or you do not want to use them while querying, you should disable quotes in Kafka Connect with:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"quote.sql.identifiers=never\n"})}),"\n",(0,t.jsx)(n.h2,{id:"deploy-new-connectors-in-k8s",children:"Deploy new connectors in K8s"}),"\n",(0,t.jsx)(n.p,{children:'Deploying new connectors can be tricky, especially if you are using Kubernetes. Kafka Connect exposes an API that we can use to create new connectors, but we have to "manually" do some calls to create, update, or delete connectors. This is not the best way of integrating something on our CI/CD, especially if our CI is running outside our K8s cluster.'}),"\n",(0,t.jsx)(n.p,{children:"Ideally, we would want to have a configuration file in our repository, that can be updated and automatically deployed during our CI."}),"\n",(0,t.jsx)(n.h2,{id:"connect-operator",children:"connect-operator"}),"\n",(0,t.jsxs)(n.p,{children:["There is a solution for this, Confluent's ",(0,t.jsx)(n.a,{href:"https://github.com/confluentinc/streaming-ops/tree/main/images/connect-operator",children:"connect-operator"}),", although the solution is not very robust, it does the job."]}),"\n",(0,t.jsxs)(n.p,{children:["This is based on the ",(0,t.jsx)(n.a,{href:"https://github.com/flant/shell-operator",children:"shell-operator"}),', a Kubernetes operator that can be deployed in our cluster and configured to "listen" for specific events, like new deployments, changes in config maps or whatever we want.\nSpecifically, this ',(0,t.jsx)(n.code,{children:"connect-operator"})," is designed to listen for changes in a config map, and then it will create, update, or delete connectors based on the content of that config map."]}),"\n",(0,t.jsxs)(n.p,{children:["In other words, we can put our Connector's configuration in a config map, and then the ",(0,t.jsx)(n.code,{children:"connect-operator"})," will create the connector for us."]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"connect-operator"})," does not need to be deployed together with Kafka Connect, it is an independent pod that will be running in our cluster,\nit can be in the same namespace or not. It also can listen for config-maps attached to our Kafka Connect or to any other deployment,\nall depending on our configuration and K8s permissions."]})}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"connect-operator"})," is a nice tool that does the job, but it isn't very robust. For example, it does not check if a connector creation has failed or not, it only checks if the connector exists or not, sends a ",(0,t.jsx)(n.code,{children:"curl"})," to the REST API, and then it assumes that everything is fine.\nIn any case, it is just a bash script using JQ for configuration, so it can be easily modified to fit our needs."]})}),"\n",(0,t.jsx)(n.h3,{id:"configuring-the-connect-operator",children:"Configuring the connect-operator"}),"\n",(0,t.jsxs)(n.p,{children:["As the ",(0,t.jsx)(n.code,{children:"connect-operator"})," is based on the ",(0,t.jsx)(n.code,{children:"shell-operator"}),", it expects a configuration file in YAML format, where we can define the events we want to listen to."]}),"\n",(0,t.jsx)(n.p,{children:"By default, the operator is called in two ways:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["At startup, it will be called with a flag ",(0,t.jsx)(n.code,{children:"--config"})," and it has to return the configuration file in YAML format that specifies the events we want to listen to."]}),"\n",(0,t.jsx)(n.li,{children:"When an event is triggered, our script will be triggered with the event as a parameter."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"listening-to-config-maps",children:"Listening to config maps"}),"\n",(0,t.jsx)(n.p,{children:"The config that we have to return to listen for config map changes should see something similar to this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'configVersion: v1\nkubernetes:\n- name: ConnectConfigMapMonitor\n  apiVersion: v1\n  kind: ConfigMap\n  executeHookOnEvent: ["Added","Deleted","Modified"]\n  jqFilter: ".data"\n  labelSelector:\n    matchLabels:\n      destination: $YOUR_DESTINATION\n  namespace:\n    nameSelector:\n      matchNames: ["$YOUR_NAMESPACE"]\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"YOUR_DESTINATION"})," must be the same as the label used in the config map, and ",(0,t.jsx)(n.code,{children:"YOUR_NAMESPACE"})," must be the same as the namespace where the config map is deployed."]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["The default ",(0,t.jsx)(n.code,{children:"connect-operator"})," has a config to enable or disable the connector, but it is done in a way that will enable or disable all your connectors at once, so I prefer to skip that part as I want to have multiple connectors in my config maps."]})}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["The configuration looks different from a standard K8s configuration, but the ",(0,t.jsx)(n.code,{children:"shell-operator"})," can handle it and ",(0,t.jsxs)(n.strong,{children:["there is no need to declare a new ",(0,t.jsx)(n.a,{href:"https://helm.sh/docs/chart_best_practices/custom_resource_definitions/",children:"CRD"})," with that structure."]})]})}),"\n",(0,t.jsx)(n.h3,{id:"config-map",children:"Config Map"}),"\n",(0,t.jsx)(n.p,{children:"The config map must have the connectors' configuration in JSON, in the same way, you will use it in the REST API.\nI would suggest building a Helm template for config maps, so you can write your connectors configuration in YAML and then convert it to JSON using Helm."}),"\n",(0,t.jsx)(n.p,{children:"Something like this should work in Helm:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-gotemplate",children:"{{- if .Values.configMap }}\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    destination: {{ .Values.your-deployment-name }} # this has to match with the label in the connect-operator config\ndata:\n  {{- range $key, $value := .Values.configMap.data }}\n    {{ $key }}: {{ $value | toJson | quote | indent 6 | trim }}\n  {{- end }}\n{{- end }}\n"})}),"\n",(0,t.jsx)(n.p,{children:"After having this Helm template, we can write our Connector's config like this:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'configMap:\n  data:\n    my-connector-name:\n      name: "my-connector-name"\n      config:\n        # JDBC Config\n        name: "my-connector-name"\n        connector.class: "io.confluent.connect.jdbc.JdbcSinkConnector"\n        # using IAM Auth\n        connection.url: "jdbc:aws-wrapper:postgresql://{host}:{port}/postgres?wrapperPlugins=iam&wrapperDialect=rds-pg"\n        connection.user: env.USERNAME\n        dialect.name: "PostgreSqlDatabaseDialect"\n        topics: "my-topic"\n        tasks.max: "4"\n        # ...\n'})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["The config-map can be attached to your Kafka Connect deployment, or to any other deployment, what matters is that the ",(0,t.jsx)(n.code,{children:"connect-operator"})," can find it."]})}),"\n",(0,t.jsxs)(n.p,{children:["Once it is deployed as a config-map, the ",(0,t.jsx)(n.code,{children:"connect-operator"})," will create the connector for us."]}),"\n",(0,t.jsx)(n.h3,{id:"replacing-variables",children:"Replacing variables"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"connect-operator"})," uses ",(0,t.jsx)(n.code,{children:"jq"})," to replace variables, they can be stored in some config files inside the docker, passed as arguments, or as environment variables.\nHaving them inside config files inside the Docker images looks weird to me, why our operator should know about the context of our connectors?"]}),"\n",(0,t.jsxs)(n.p,{children:["These are some examples of replacing variables, but you can find more details in the ",(0,t.jsx)(n.code,{children:"jq"})," documentation:"]}),"\n",(0,t.jsx)(n.p,{children:"Variables:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"connection.user: $username\n"})}),"\n",(0,t.jsx)(n.p,{children:"Environment variables:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"connection.user: env.USERNAME\n"})}),"\n",(0,t.jsx)(n.p,{children:"Note that they are not between quotes."}),"\n",(0,t.jsx)(n.p,{children:"Variables inside strings:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'connection.user: "prefix_\\(env.USERNAME)_suffix"\n'})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["If you are parsing this with Helm, you might need to have the string between single quotes, otherwise, Helm will fail on ",(0,t.jsx)(n.code,{children:"\\("})]})}),"\n",(0,t.jsx)(n.h3,{id:"rbac-permissions-to-read-config-maps",children:"RBAC permissions to read config maps"}),"\n",(0,t.jsxs)(n.p,{children:["If your ",(0,t.jsx)(n.code,{children:"connect-operator"})," stays in a different deployment than the config-map, you will need to give it permission to read the config map. This can be achieved using a Role and a RoleBinding using Helm."]}),"\n",(0,t.jsx)(n.p,{children:"Something like this needs to be created:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: {{ .Values.your-app }}-configmap-read-role\n  namespace: {{ .Release.Namespace }}\nrules:\n  - apiGroups: [""]\n    resources: ["configmaps"]\n    verbs: ["list", "watch"] # List and Watch all configmaps, get only the ones specified in resourceNames\n  - apiGroups: [""]\n    resources: ["configmaps"]\n    resourceNames:\n      ["your-destination"] # this is the deployment having the config-map\n    verbs: ["get", "watch", "list"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: {{ .Values.your-app }}-read-configmaps\n  namespace: {{ .Release.Namespace }}\nsubjects:\n  - kind: ServiceAccount\n    name: {{ .Values.your-app }}\nroleRef:\n  kind: Role\n  name: {{ .Values.your-app }}-configmap-read-role\n  apiGroup: rbac.authorization.k8s.io\n{{- end }}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"custom-kafka-groups-for-your-connectors",children:"Custom Kafka groups for your connectors"}),"\n",(0,t.jsxs)(n.p,{children:["By default, Kafka Connect will create a group for each connector, and it will use the connector's name as the group name, with ",(0,t.jsx)(n.code,{children:"connect-"})," as a prefix.\nThis is not very flexible, as we might want to have our own group names. For example, if we are sharing K8s clusters with other teams,\nwe might want to have our own group names to avoid conflicts. Or we could have our own naming convention with ACLs in Kafka."]}),"\n",(0,t.jsx)(n.p,{children:"To decide a group name, we have to change two configurations:"}),"\n",(0,t.jsx)(n.p,{children:"First, we have to create an environment variable that allows us to override some configs, including the group name:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY=All\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"This can be on your deployment file or on your Dockerfile."})}),"\n",(0,t.jsx)(n.p,{children:"Then, we can add the group name to our connector's configuration:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'consumer.override.group.id: "my-custom-group-name"\n'})})]})}function h(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},11151:(e,n,i)=>{i.d(n,{Z:()=>r,a:()=>s});var t=i(67294);const a={},o=t.createContext(a);function s(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);