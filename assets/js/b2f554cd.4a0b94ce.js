"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"kafka-reassign-partitions","metadata":{"permalink":"/blog/kafka-reassign-partitions","editUrl":"https://github.com/JavierMonton/blog/edit/main/website/blog/2024-10-06-kafka-reassign-partitions/index.md","source":"@site/blog/2024-10-06-kafka-reassign-partitions/index.md","title":"Kafka - Reassign Partitions","description":"When working with Kafka, increasing or decreasing the number of brokers isn\'t as trivial as it seems. If you add a new broker,","date":"2024-10-06T00:00:00.000Z","formattedDate":"October 6, 2024","tags":[{"label":"partitions","permalink":"/blog/tags/partitions"},{"label":"kafka-binaries","permalink":"/blog/tags/kafka-binaries"},{"label":"MSK","permalink":"/blog/tags/msk"}],"readingTime":3.515,"hasTruncateMarker":false,"authors":[{"name":"Javier Mont\xf3n","title":"Software Engineer","url":"https://github.com/JavierMonton","imageURL":"https://github.com/JavierMonton.png","key":"javier"}],"frontMatter":{"slug":"kafka-reassign-partitions","title":"Kafka - Reassign Partitions","authors":"javier","tags":["partitions","kafka-binaries","MSK"]},"unlisted":false,"nextItem":{"title":"Kafka Connect, MM2, and Offset Management","permalink":"/blog/kafka-connect-mm2-offset-management"}},"content":"When working with Kafka, increasing or decreasing the number of brokers isn\'t as trivial as it seems. If you add a new broker, \\nit will stand there doing nothing. You have to manually reassign partitions of your topics to the new broker. \\nBut you don\'t want to just move some topics completely to your new broker, you want to spread your partitions and their replicas equitably across all your brokers. \\nYou also want to have the number of leader partitions balanced across all your brokers.\\n\\n# Reassign partitions\\nTo reassign partitions to different brokers, you can use the Kafka binaries (`bin/kafka-reassign-partitions.sh`), \\nbut it isn\'t trivial if you have to reassign thousands of topics.\\n\\nThe binary file has three operations: \\n- `--generate`. This will generate a plan to reassign partitions.\\n- `--execute`. This will execute the plan.\\n- `--verify`. This will verify the status of the reassignment.\\n\\n:::tip\\nA throttle can be set to avoid overloading the brokers, and the throttle will remain in the cluster after the reassignment, \\nuntil a `--verify` is run when the reassignment has finished, so it\'s highly recommended to run `--verify` \\nuntil you are sure all the partitions have been reassigned.\\n:::\\n\\n\\nTo create a plan, you have to pass a JSON file with the topics you want to reassign and the brokers you want to reassign them to.\\ne.g.: \\n```json\\n{\\n  \\"topics\\": [\\n    { \\"topic\\": \\"foo1\\" },\\n    { \\"topic\\": \\"foo2\\" }\\n  ],\\n  \\"version\\": 1\\n}\\n```\\nAnd it will generate you a file like this:\\n```json\\n{\\"version\\":1,\\n  \\"partitions\\":[{\\"topic\\":\\"foo1\\",\\"partition\\":0,\\"replicas\\":[2,1],\\"log_dirs\\":[\\"any\\"]},\\n    {\\"topic\\":\\"foo1\\",\\"partition\\":1,\\"replicas\\":[1,3],\\"log_dirs\\":[\\"any\\"]},\\n    {\\"topic\\":\\"foo1\\",\\"partition\\":2,\\"replicas\\":[3,4],\\"log_dirs\\":[\\"any\\"]},\\n    {\\"topic\\":\\"foo2\\",\\"partition\\":0,\\"replicas\\":[4,2],\\"log_dirs\\":[\\"any\\"]},\\n    {\\"topic\\":\\"foo2\\",\\"partition\\":1,\\"replicas\\":[2,1],\\"log_dirs\\":[\\"any\\"]},\\n    {\\"topic\\":\\"foo2\\",\\"partition\\":2,\\"replicas\\":[1,3],\\"log_dirs\\":[\\"any\\"]}]\\n}\\n```\\nThe input expects you to give the list of brokers (1,2,3,4,5...) and this JSON with the whole list of partitions and replicas.\\n\\n:::warning\\nThe first number in `\\"replicas\\":[1,3]` is the leader partition, the rest are the followers. \\nThis is very important because you might end up with more leader partitions in a broker than others, increasing its workload\\n:::\\n\\n## Problem with Kafka Reassign tool\\n\\nWhen you need to reassign a big cluster, you might find some issues with the `--generate` command:\\n\\n- The plan generated is completely random. Running it twice for a topic will produce different results.\\n- The plan generated is not always optimal. It might assign more partitions to one broker than to another.\\n\\nOn a cluster with thousands of partitions this might be ok, as probably randomizing the partitions will be enough to balance them across the brokers,\\nbut it might not be completely optional.\\nAlso, if you need to run this for a lot of topics, and you want to do it on batches, you don\'t want to run the `--generate` command for a topic twice, \\nin that case, you will be reassigning a topic that was already reassigned.\\n\\n## Building a custom reassign plan.\\n\\nTo manage partitions more properly, a custom tool can be built, where partitions are defined based on the topic name and the list of brokers. \\nBy doing this, reassigning partitions on the same topic twice won\'t produce any changes. A tool like that can be used to manage the reassignment of partitions in a more controlled way.\\n\\n# Balance Leader partitions\\nAfter reassigning a lot of partitions, the leader partitions might not be well-balanced across your brokers. \\nThis means that a broker might have more leader partitions than other brokers, which is translated into more workload.\\nAn example in Kafka-UI:\\n![img.png](unbalanced-leaders.png)\\n\\nIf you wait, the cluster probably will rebalance the leader partitions on its own (if `auto.leader.rebalance.enable=true` is set).\\n\\nIn order to force a rebalance, you can use the `bin/kafka-leader-election.sh` binary.\\n\\nThis an example of the CPU usage of brokers before and after the leader election:\\n![img.png](cpu.png)\\n\\ne.g.:\\n```shell\\n$ bin/kafka-leader-election.sh --bootstrap-server localhost:9092 --election-type preferred --all-topic-partitions\\n```\\n`--election-type` can be `preferred` or `unclean`. `preferred` will try to move the leader partition to the preferred broker, `unclean` will move the leader partition to any broker.\\n\\n\\n# TL;DR\\n\\nTo reassign partitions to new brokers:\\n\\n- Use the `bin/kafka-reassign-partitions.sh` with a list of topics, brokers and the `--generate` command.\\n- Use the `bin/kafka-reassign-partitions.sh` with the generated JSON and the `--execute` command.\\n- Use the `bin/kafka-reassign-partitions.sh` with the generated JSON and the `--verify` command.\\n- Use the `bin/kafka-leader-election.sh` to balance the leader partitions across your brokers."},{"id":"kafka-connect-mm2-offset-management","metadata":{"permalink":"/blog/kafka-connect-mm2-offset-management","editUrl":"https://github.com/JavierMonton/blog/edit/main/website/blog/2024-04-13-kafka-connect-mm2-offset-management/index.md","source":"@site/blog/2024-04-13-kafka-connect-mm2-offset-management/index.md","title":"Kafka Connect, MM2, and Offset Management","description":"This post is about Kafka Connect, Mirror Maker 2, how they manage offsets, and how to deal with them.","date":"2024-04-13T00:00:00.000Z","formattedDate":"April 13, 2024","tags":[{"label":"Scala","permalink":"/blog/tags/scala"},{"label":"Java","permalink":"/blog/tags/java"},{"label":"Kafka","permalink":"/blog/tags/kafka"},{"label":"Kafka-Connect","permalink":"/blog/tags/kafka-connect"},{"label":"MM2","permalink":"/blog/tags/mm-2"},{"label":"offsets","permalink":"/blog/tags/offsets"},{"label":"MSK","permalink":"/blog/tags/msk"},{"label":"Mirror-Maker2","permalink":"/blog/tags/mirror-maker-2"}],"readingTime":6.35,"hasTruncateMarker":false,"authors":[{"name":"Javier Mont\xf3n","title":"Software Engineer","url":"https://github.com/JavierMonton","imageURL":"https://github.com/JavierMonton.png","key":"javier"}],"frontMatter":{"slug":"kafka-connect-mm2-offset-management","title":"Kafka Connect, MM2, and Offset Management","authors":"javier","tags":["Scala","Java","Kafka","Kafka-Connect","MM2","offsets","MSK","Mirror-Maker2"]},"unlisted":false,"prevItem":{"title":"Kafka - Reassign Partitions","permalink":"/blog/kafka-reassign-partitions"},"nextItem":{"title":"Kafka Connect - JDBC Sink","permalink":"/blog/kafka-connect-jdbc-sink-iam-auth"}},"content":"import Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n\\nThis post is about Kafka Connect, Mirror Maker 2, how they manage offsets, and how to deal with them.\\n\\n## Kafka Offsets\\n\\nWhen a consumer starts consuming messages from Kafka, it will probably use a `consumer-group` and Kafka will store the \\noffset of the last message consumed by that consumer-group. This offset is stored in a Kafka topic called `__consumer_offsets`.\\n\\nBy doing this, when the consumer restarts, it can start consuming messages from the last offset it consumed.\\n\\nThere are tools that allow us to manage these offsets, like the binary files provided by Kafka, and in any case, \\nthe consumer can always decide which offsets to consume from. This means that the consumer can set the offset to the beginning, \\nthe end, or any other offset. A lot of tools allow even to search offsets based on timestamps.\\n\\n### Monitoring offset lag\\n\\nThe `consumer-offset-lag` is a metric provided by Kafka, based on the difference between the last offset consumed by the consumer \\nand the last offset produced by the producer. Most of the monitoring tools will have this value, like DataDog, \\nand it is very useful to know if the consumer is lagging, meaning that it is not consuming messages as fast as they are produced, or it is down.\\n\\nBut ****Mirror Maker 2 (MM2) can not be monitored through this metric****, see below.\\n\\n## Kafka Connect Offsets\\n:::note\\nWhen we talk about Kafka Connect, we are talking about the distributed version of Kafka Connect, not the standalone version.\\n:::\\n\\nKafka Connect manage offsets in their own way. When a consumer is started by Kafka Connect, it will store the offsets in a Kafka topic \\ncalled `connect-offsets` by default, although it can be configured through the `offset.storage.topic` property.\\n\\n:::tip\\nWhen a connector is created in Kafka Connect, it will track the offsets in its own topic, but it also will use a regular \\"consumer-group\\" so the offsets will be also tracked in `__consumer_offsets`. \\nThis also means that we can monitor Kafka Connect sinks through the `consumer-offset-lag` metric.\\n:::\\n\\n\\n## Mirror Maker 2 (MM2) Offsets\\n\\nMirror Maker 2 is a tool provided by Kafka to replicate messages from one Kafka cluster to another, it is meant to be used for a complete replication between clusters, \\nbut it can be used to copy only some topics.\\n\\n:::note\\nMM2 can be run in different ways, but here we are talking about the `Kafka Connect` way, where MM2 is run as a Kafka Connect connector.\\n:::\\n\\nIf we think about MM2 and the data it needs to consume and produce, we can have doubts about how offsets can be managed. \\nTo start with, it needs to read in one cluster and write on another, so, how does it manage the offsets?\\n\\nBy default, MM2 will create a topic called `mm2-offset-syncs.<cluster-alias>.internal` and as far asI know, it can not be renamed.\\n\\n:::tip\\nWhile working with MM2, it is recommended to install the connectors in the \\"target\\" cluster, so the \\"source\\" cluster will be the external one.\\n:::\\n\\nBy default, MM2 will create the aforementioned topic in the \\"source\\" cluster, and it will store the offsets of the last message consumed and produced. \\nBut as we can see, the \\"source\\" cluster is \\"external\\" to where the Kafka Connect is installed, and that might cause some issues \\nin cases where the \\"source\\" cluster is not managed by us. For example, we might not have write or create access, and we can not create the topic.\\n\\nThe destination of `mm2-offset-syncs.<cluster-alias>.internal` can be defined by the [`offset-syncs.topic.location` property](https://kafka.apache.org/documentation/#mirror_source_offset-syncs.topic.location) which accepts `source` (default) and `target`.\\n\\n:::note\\nWhen a Consumer is created by MM2, which is a Kafka Connect connector, it will store the offsets both in `mm2-offset-syncs.<cluster-alias>.internal` and in `connect-offsets`.\\nThis is very important if we want to manipulate offsets\\n:::\\n\\n:::warning\\nMM2 consumers do not use a `group.id`, they do not use any Kafka consumer group and their consumed offset won\'t be stored in `__consumer_offsets`. \\nThis also means that we can not monitor MM2 through the `consumer-offset-lag` metric.\\n:::\\n\\n## Mixing Kafka Connect and MM2\\n\\nIf we look at the offsets stored both by Kafka Connect and MM2 in their topics, we can see the following:\\n\\n### Kafka Connect topic\\n\\nIf we look at the `connect-offsets` topic, we can see that the offsets are stored in JSON format, with the following structure:\\n- `key` is a structure that contains the connector name, the partition, the topic, and the cluster.\\n```json\\n[\\n\\t\\"my-connector-name\\",\\n\\t{\\n\\t\\t\\"cluster\\": \\"my-source-cluster-alias\\",\\n\\t\\t\\"partition\\": 3,\\n\\t\\t\\"topic\\": \\"my-example-topic\\"\\n\\t}\\n]\\n```\\n- And the `value` is a JSON with the offset:\\n```json\\n{\\n    \\"offset\\": 1234\\n}\\n```\\n:::note\\nNo matter where we store the offsets (source or target), Kafka Connect will show the \\"source cluster alias\\" as this is where the Kafka consumer is created.\\n:::\\n\\n### MM2 topic\\n\\nIf we look at the `mm2-offset-syncs.<cluster-alias>.internal` topic, we can see KC uses its own format to store the offsets:\\n- `key` is the connector name, but it has a few extra bytes, which represents some structure defined inside the code\\n- `value` is just an Int64, which represents the offset\\n\\nManaging offsets is not really recommended as we could mess up the connectors, but it is possible to do it.\\n\\n\\n## Hot to reset offsets in Mirror Maker 2\\n\\nIf we need to reset the offsets in MM2, we might think that deleting the topic `mm2-offset-syncs.<cluster-alias>.internal` will do the trick, \\nbut it won\'t, as offsets are also stored in Kafka Connect\'s topic. So, we need to reset the offsets in both topics.\\n\\n\\nThere is a lot of misinformation about how to reset the offsets in Kafka Connect, their docs are not very clear about it, and Kafka Connect has been lacking tools to work with it. \\nTypically, removing the connector and creating it with a different name will do the trick, but we might want to keep the same name.\\n\\n\\n### Manual edit of offsets\\n\\nWe can manually produce a message in the `connect-offsets` topic to reset offsets, and the right way of doing it is to send a `tombstone`. \\nWe can check the messages we have right now, identify the connector we want and send the same Key with `null` value.\\n\\n:::note\\nTo reset offsets completely we do not specify `offset: 0`, we send a null value\\n:::\\n\\n### REST API to reset offsets\\nStarting from [Kafka 3.6.0](https://archive.apache.org/dist/kafka/3.6.0/RELEASE_NOTES.html), Kafka Connect has a REST API to manage connectors, and it is possible to reset offsets through it.\\nThe docs about it are defined in the [KPI-875](https://cwiki.apache.org/confluence/display/KAFKA/KIP-875%3A+First-class+offsets+support+in+Kafka+Connect#KIP875:FirstclassoffsetssupportinKafkaConnect-PublicInterfaces), but they are still not present in the official docs.\\nIf you are using Confluent, starting from [Confluent\'s 7.6.0 version](https://docs.confluent.io/platform/current/release-notes/index.html) Kafka 3.6.0 is included.\\n\\nIf we use this version, we can simply do a few curls to reset offsets. First we need to stop the connector and then reset the offsets.\\n\\n```bash\\ncurl -X PUT http://localhost:8083/connectors/my-connector-name/offsets/stop\\ncurl -X DELETE http://localhost:8083/connectors/my-connector-name/offsets\\n```\\n\\nWe can also know the status of the offsets:\\n```bash\\ncurl -X GET http://localhost:8083/connectors/my-connector-name/offsets | jq\\n```\\n\\n## TL;DR\\nTo reset offsets in MM2, you need to:\\n- Stop, pause or remove the connectors\\n- Delete or truncate the topic `mm2-offset-syncs.<cluster-alias>.internal` \\n- Reset the offsets in the `connect-offsets` topic, either manually or through the REST API for the desired connector\\n- Start the connectors again\\n\\n:::warning\\nDeleting the topic `mm2-offset-syncs.<cluster-alias>.internal` will not reset the offsets for other connectors you have configured in MM2 \\nas they fall back to the `connect-offsets` topic, but be careful and do this at your own risk, things might change in the future and this could become false.\\n:::"},{"id":"kafka-connect-jdbc-sink-iam-auth","metadata":{"permalink":"/blog/kafka-connect-jdbc-sink-iam-auth","editUrl":"https://github.com/JavierMonton/blog/edit/main/website/blog/2023-12-16-kafka-connect-jdbc-sink/index.md","source":"@site/blog/2023-12-16-kafka-connect-jdbc-sink/index.md","title":"Kafka Connect - JDBC Sink","description":"A guide to move data from Kafka to an AWS RDS using Kafka Connect and the JDBC Sink Connector with IAM Auth.","date":"2023-12-16T00:00:00.000Z","formattedDate":"December 16, 2023","tags":[{"label":"Kafka-Connect","permalink":"/blog/tags/kafka-connect"},{"label":"JDBC-Sink","permalink":"/blog/tags/jdbc-sink"},{"label":"Kafka","permalink":"/blog/tags/kafka"},{"label":"IAM-Auth","permalink":"/blog/tags/iam-auth"},{"label":"MSK","permalink":"/blog/tags/msk"},{"label":"K8s","permalink":"/blog/tags/k-8-s"}],"readingTime":19.33,"hasTruncateMarker":false,"authors":[{"name":"Javier Mont\xf3n","title":"Software Engineer","url":"https://github.com/JavierMonton","imageURL":"https://github.com/JavierMonton.png","key":"javier"}],"frontMatter":{"slug":"kafka-connect-jdbc-sink-iam-auth","title":"Kafka Connect - JDBC Sink","authors":"javier","tags":["Kafka-Connect","JDBC-Sink","Kafka","IAM-Auth","MSK","K8s"]},"unlisted":false,"prevItem":{"title":"Kafka Connect, MM2, and Offset Management","permalink":"/blog/kafka-connect-mm2-offset-management"},"nextItem":{"title":"Big Data Types Library","permalink":"/blog/big-data-types-library"}},"content":"A guide to move data from Kafka to an AWS RDS using Kafka Connect and the JDBC Sink Connector with IAM Auth.\\n\\n## Kafka Connect\\nFor these examples, we are using Confluent\'s Kafka Connect on its Docker version, as we are going to deploy it in a Kubernetes cluster.\\n\\n### Single and distributed modes\\n\\nKafka Connect comes with two modes of execution, single and distributed. The main difference between them is that the single mode runs all the connectors in the same JVM, while the distributed mode runs each connector in its own JVM. The distributed mode is the recommended one for production environments, as it provides better scalability and fault tolerance. \\nIn the case of K8s, it means we will be using more than one pod to run Kafka Connect.\\n\\n:::warning\\nBe aware that these two modes use different class paths, so if you are doing changes inside the docker and you are running the single mode locally but distributed in production, you might have different results.\\nI strongly recommend to check manually which are the class paths in each case using something like \\n```bash\\nps aux | grep java\\n```\\nAnd  you will get something like this:\\n```bash\\njava -Xms256M -Xmx2G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true -Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/var/log/kafka -Dlog4j.configuration=file:/etc/kafka/connect-log4j.properties -cp /etc/kafka-connect/jars/*:/usr/share/java/kafka/*:/usr/share/java/confluent-common/*:/usr/share/java/kafka-serde-tools/*:/usr/share/java/monitoring-interceptors/*:/usr/bin/../share/java/kafka/*:/usr/bin/../share/java/confluent-telemetry/* org.apache.kafka.connect.cli.ConnectDistributed /etc/kafka-connect/kafka-connect.properties\\n```\\nAnd you\'ll find all the directories (after `-cp`) included in the running Kafka Connect. \\n\\n- Note that a folder called `cp-base-new` is widely used in the Single mode, but is not very well documented.\\n- Setting your deployment to 1 replicas will run Kafka Connect in Single mode while setting it to 2 or more will run it in Distributed mode.\\n:::\\n\\n### Deploying in K8s\\nThis should be fairly straightforward, as we are using Confluent\'s Kafka Connect Docker image, which is already prepared to be deployed in K8s.\\nConfluent provides a [Helm chart](https://github.com/confluentinc/cp-helm-charts/blob/master/charts/cp-kafka-connect/README.md) as an example, so it should be easy. You can also create your own.\\n\\n### Using MSK (Kafka)\\nIf you are using the AWS\'s Kafka version, MSK, and you are authenticating using IAM, you will need to do a few things:\\n- Configure some environment variables in Kafka Connect \\n- Add the required AWS libraries to the classpath\\n\\n#### Environment variables\\n`CONNECT_BOOTSTRAP_SERVERS` will have the brokers, as usual, but using the `9098` port.\\n\\nYou need to specify the IAM callback handler as well as SASL:\\n```bash\\nCONNECT_SASL_CLIENT_CALLBACK_HANDLER_CLASS = software.amazon.msk.auth.iam.IAMClientCallbackHandler\\nCONNECT_SASL_MECHANISM = AWS_MSK_IAM\\nCONNECT_SECURITY_PROTOCOL = SASL_SSL\\n```\\nAlso, you have to provide a `JAAS` file with the credentials. You can find more info about this in the [AWS\'s documentation](https://docs.aws.amazon.com/msk/latest/developerguide/msk-password.html#msk-password-sasl-plain).\\nFor IAM, something like this should work:\\n```bash\\nCONNECT_SASL_JAAS_CONFIG = \\n      software.amazon.msk.auth.iam.IAMLoginModule required\\n      awsRoleArn=\\"arn:aws:iam::{account}:role/{role}\\"\\n      awsStsRegion=\\"{region}\\";\\n```\\nIf you do this in yaml for Helm, it will look like this:\\n\\n```yaml\\n  - name: CONNECT_SASL_JAAS_CONFIG\\n    value: >-\\n      software.amazon.msk.auth.iam.IAMLoginModule required\\n      awsRoleArn=\\"arn:aws:iam::{account}:role/{role}\\"\\n      awsStsRegion=\\"{region}\\";\\n```\\n\\nWhen Kafka Connect creates a new connector, it will use its own credentials configuration, so if you want to have the same IAM auth, you will need to add the same values to these environment variables:\\n- `CONNECT_CONSUMER_SASL_CLIENT_CALLBACK_HANDLER_CLASS`\\n- `CONNECT_CONSUMER_SASL_MECHANISM`\\n- `CONNECT_CONSUMER_SECURITY_PROTOCOL`\\n- `CONNECT_CONSUMER_SASL_JAAS_CONFIG`\\n\\n:::tip\\nIf you are using your own Helm template, you could create some variables for these values, so you can reuse them in the different environment variables, to avoid writing them twice.\\n:::\\n  \\n### Formatting logs as JSON\\nLogs are very important, and having a good format is key to being able to read and process them easily. Usually, in production, we could want to have them as JSON, and Kafka Connect does not make it as easy for us as we might expect.\\n\\nIf you only want to change the log level or format your logs a bit, you could use the environment variables available for that, they are [described in their docs](https://docs.confluent.io/platform/current/connect/logging.html#use-environment-variables-docker)\\nbut if you want to proper format all logs as JSON, you will need to do a few more things.\\n\\n#### Using JSONEventLayoutV1\\nKafka Connect uses `log4j1` (not `log4j2`), so we will need to use a `log4j.properties` file to configure it. They are using a patched version of the original Log4j1 that is supposed to fix some vulnerabilities.\\n\\nWe can use a dependency that automatically converts all of our logs into JSON, like [log4j-jsonevent-layout](https://github.com/logstash/log4j-jsonevent-layout). See [Adding libraries](#adding-libraries) for more info about how to add new libraries.\\nIf we have this library in the classpath, we can now use the `JSONEventLayoutV1` in our `log4j.properties` file. Like:\\n\\n```properties\\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\\nlog4j.appender.stdout.layout=net.logstash.log4j.JSONEventLayoutV1\\n```\\n\\n#### Properties files\\nConfluent will tell you that you can modify the template for logs in `/etc/confluent/docker/log4j.properties.template`, but you might need some extra steps if you want __all__ logs as JSON.\\n\\n- Template for most of the logs, as described, in `/etc/confluent/docker/log4j.properties.template`\\n- Logs from the \\"Admin Client\\" in `/etc/kafka/log4j.properties`\\n- Some tool logs in `/etc/confluent/docker/tools-log4j.properties.template`\\n- Some startup logs in `/etc/kafka-connect/log4j.properties`\\n- There are also some random logs not using Log4j, they are defined in `/usr/lib/jvm/jre/conf/logging.properties`\\n\\nIf you want to format everything to JSON, I would recommend entering inside the docker image, looking for those files, and changing them as desired. Your Dockerfile could then replace them while creating the image.\\n\\n:::warning\\nThere are still some logs during the start-up not formatted as JSON. Confluent\'s Kafka Connect is using a [Python script to start up the service](https://github.com/confluentinc/confluent-docker-utils/blob/master/confluent/docker_utils/cub.py), \\nand that service is using some `prints` that do not belong to any Log4j, so they are not formatted in any way.\\n\\nIf you want to format those `prints` too, you will need to do something else as they don\'t have any configuration files. You could use a `sed` command to replace them, or you could modify the `cub.py` file in your image with the desired format.\\n:::\\n\\n\\n### Adding plugins\\nAdding plugins should be straightforward, the documentation explains pretty well how to do it. Note that you can add plugins inside the plugins folder or you could modify the plugins folder with \\n```\\nplugin.path=/usr/local/share/kafka/plugins\\n```\\n\\nIn any case, copying and pasting files into a Docker can limit a bit the flexibility of the solution, so I would recommend building a project where you can add all the dependencies that you need, meaning that libraries and plugins can be built and copied inside the Docker during the CI. \\nBy doing this, you will be able to use Gradle, Maven, SBT, or any other building tool to manage your dependencies, upgrade versions, and build plugins.\\n\\n:::tip\\nNote that Plugins and libraries are not included in the same path, so I would recommend building a different project for each. \\nFor example, we could build a main project that can build the Kafka Connect image with their libraries and a subproject that can build plugins in a different folder. Then, the Dockerfile could easily copy both folders into the image in the right paths.\\n:::\\n\\nIf you build a project like that, to add the JDBC plugin, for example, in Gradle you only need to add this:\\n```kotlin\\ndependencies {\\n    implementation(\\"io.confluent:kafka-connect-jdbc:10.7.4\\")\\n}\\n```\\n\\n### Adding libraries\\nAs mentioned earlier, libraries must go in the classpath, not in the plugins\' folder. \\nIf you are using a project to build your libraries and plugins, you could use many different plugins to pack all the dependencies into a .jar that can be copied into the Docker image.\\n\\nFor example, with Gradle, we could include the AWS library needed for IAM authentication, and the Log4j JSON formatter, like this:\\n```kotlin\\ndependencies {\\n    implementation(\\"software.amazon.msk:aws-msk-iam-auth:1.1.7\\")\\n    implementation(\\"net.logstash.log4j:jsonevent-layout:1.7\\")\\n}\\n```\\nUsing a plugin to build a fat jar, everything should be included in one .jar file that we can copy into the Docker image.\\n\\n:::tip\\nFor the JDBC Sink, we will need to also include a Driver and more libraries in case we want to use IAM Auth with RDS, we will see that later.\\n:::\\n\\n### Kafka Connect REST API\\nBy default, Kafka Connect exposes its REST API in the port `8083`. You can find more info about the API in the [official documentation](https://docs.confluent.io/platform/current/connect/references/restapi.html).\\n\\nIf you want to control who can access the API or change its port, you can use the `CONNECT_LISTENERS` and/or `CONNECT_REST_ADVERTISED_PORT` environment variables. \\nFor example, if you want to change the port to `8084`, you could do this:\\n\\n```bash\\n  - name: CONNECT_REST_ADVERTISED_PORT\\n    value: \\"8084\\"\\n```\\nAlso, you can even open the API in multiple ports, by doing this:\\n```bash\\n  - name: CONNECT_LISTENERS\\n    value: \\"http://0.0.0.0:8084,http://0.0.0.0:8085\\"\\n  - name: CONNECT_REST_ADVERTISED_PORT\\n    value: \\"8084\\"\\n```\\n\\n#### Securing the API\\nKafka Connect\'s REST API lacks security options, it only allows you to use a basic authentication, which might not be what you are looking for. Also, the code seems to have several places where they do an `if - else` to check if basic auth is enabled or not.\\n\\nBut, there is also another way we can use to build our own security layer. \\n\\n##### JAX-RS Security Extensions\\nWithout entering too much into details, Kafka Connect, as well as Schema Registry, are using [JAX-RS](https://en.wikipedia.org/wiki/Jakarta_RESTful_Web_Services) to build their REST APIs, and JAX-RS allows us to add our own security extensions.\\nFollowing this pattern, we could add a simple filter to check if the user is authenticated or not, and if not, we could return a `401` error.\\n\\nAbout how to authenticate a user, we could use different methods, depending on our setup. For example, we could use AWS IAM API to check if a user has permissions or not, or as we are deploying this in Kubernetes, we could rely on [Kubernetes identities](https://learnk8s.io/microservices-authentication-kubernetes) which will allow us to authenticate pods using a JWT token.\\n\\nTo do this, you have to create a JAX-RS plugin and then register it in Kafka Connect. Once your plugin is ready, you can register it by extending `ConnectRestExtension`:\\n```scala\\nclass MySecurityExtension extends ConnectRestExtension {}\\n```\\nThis class will need to be packed with your libraries and included in the classpath, as we did with other libraries.\\n\\n\\n## JDBC Sink Connector\\n\\nWe need to download the plugin and add it to the plugins\' folder. By default, it\'s `/usr/share/confluent-hub-components/`. \\n\\nYou can get the .jar with `wget` and copy it inside the Docker image, in the aforementioned folder. Or, as suggested earlier, if you are building a project using a building tool, like Gradle, you can use Maven to download all the plugins you might need. We only need to add the dependency:\\n```kotlin\\ndependencies {\\n    implementation(\\"io.confluent:kafka-connect-jdbc:10.7.4\\")\\n}\\n```\\nAnd build the .jar. Then, we can copy it inside the Docker image.\\n\\n### Drivers\\nOnly the plugin is not enough to connect to a database, we will also need the driver. In our case, we are using PostgreSQL RDS, so we will need the driver for Postgres.\\n:::info\\nSeveral drivers are already included in the Kafka Connect image, but they are not inside the default classpath, so if we try to run the connector without adding the driver properly, we will get an error like `No suitable driver found`.\\nThey are placed in `/usr/share/confluent-hub-components/`, but as we can see using something like `ps aux | grep java`, they are not included in the classpath. So, we have three options:\\n- Move the driver to the classpath\\n- Add the drivers\' folder to the classpath\\n- Find our own driver and copy it inside the Docker image, in the classpath\\n:::\\n\\nI would go for the third option, which gives us more flexibility about which version of the driver we want to use. \\n\\nSo, we can download the driver and pack it with our libraries, and then copy it inside the Docker image:\\n```kotlin\\nimplementation(\\"org.postgresql:postgresql:42.7.1\\")\\n```\\n\\n:::tip\\nNote that the JDBC Sink has to be placed in `plugins` folder, while the driver has to be placed in the `library` classpath.\\n:::\\n\\n### IAM Auth\\nIf you are using IAM Auth with RDS, you will need to add some extra libraries to the classpath. You can find more info about this in the [AWS\'s documentation](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.Connecting.Java.html).\\n\\nThe bad news is that a simple driver can not use IAM Auth, if you try to connect to the database using IAM Auth, you will get an error like `The server requested password-based authentication, but no password was provided.`. You would need to create a token manually and pass it through the connection.\\n\\nThe good news is that there is a library created by AWS that acts as a [wrapper for your JDBC Drivers](https://github.com/awslabs/aws-advanced-jdbc-wrapper), adding extra features, including IAM Auth.\\n\\nTo use IAM Auth, we only need to add this driver to the classpath, and change a bit our JDBC URL.\\n\\n\\nAdd the dependency (also needed libraries for AWS RDS):\\n```kotlin\\nimplementation(\\"com.github.awslabs:aws-advanced-jdbc-wrapper:2.3.2\\")\\nimplementation(\\"software.amazon.awssdk:rds:2.20.145\\")\\n```\\n\\nYou can follow their docs, but in our case, we will need to change the JDBC URL to add a couple of things:\\n- URL using the new driver\\n- IAM Auth enabled flag enabled\\n\\n```\\njdbc:aws-wrapper:postgresql://{host}:{port}/postgres?wrapperPlugins=iam\\n```\\n:::tip\\nSome tips:\\n- This JDBC URL goes inside the connector\'s configuration\\n- A username is still required, and it should be the same as the role used to connect to the database\\n- You can also help the wrapper to find the dialect used, by adding `&wrapperDialect=rds-pg`\\n- You can also help Kafka Connect to find the dialect used, by adding another property in your connector\'s configuration: `dialectName: \\"PostgreSqlDatabaseDialect\\"`\\n:::\\n\\n#### META-INF/services and multiple drivers\\nAt this point, we are including the JDBC PostgreSQL driver and the wrapper in the classpath, both are JDBC Drivers, if we are including different .jar files, everything should be fine,\\nbut if we are building a fat-jar, we might have some issues. Each one of these drivers is creating a file called `META-INF/services/java.sql.Driver`,\\nand they are including the name of the driver in it. If our fat-jar is not merging them to include both classes, we will get an error like `No suitable driver found`.\\n\\nDepending on the building tool and the plugin used, we might need to add some extra configuration to merge these files. For example, in Gradle we could need to add something like this:\\n```\\nmergeServiceFiles()\\n```\\nOr in the SBT Assembly plugin, we could need to add something like:\\n```sbt\\nassembly / assemblyMergeStrategy := MergeStrategy.concat\\n```\\n\\n\\n### Topic & Destination table\\nThe JDBC Sink Connector allows us to decide which topics and tables we want to use, and we have two ways of doing it:\\n- One topic / table per connector. In this case, we can directly write the topic and table names in the connector\'s configuration.\\n- Multiple topics / tables per connector. In this case, we will need to use a pattern for topics and another for tables.\\n\\n#### One topic / table per connector\\nThis is the easiest way, we only need to add the topic and table names in the connector\'s configuration, like this:\\n```yaml\\ntopics: \\"my.first.topic\\"\\ntable.name.format: \\"first_table\\"\\n```\\n\\n\\n### Using patterns for topics and table names\\nThe JDBC does some magic to map topics to tables, but it\'s not always what we want. For example, if we have a topic called `my.topic` it will take `my` as schema name and `topic` as table name. More details about [table parsing](https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/overview.html#table-parsing) can be found in their docs.\\n\\nBut, we likely use a pattern for our topics, especially if we are building a Datalake, so we might want to create tables based on a different pattern. For example, we could have a topic called `my.first.topic` and we want to create a table called `first_table` in our database. This can still be achieved using a `router` and a `table.name.format` property.\\n\\n\\n:::tip\\nBe aware that not all types accepted in your Kafka topics are accepted in your database, JDBC Driver and/or the JDBC Sink. For example, the list of valid types from the perspective of the [JDBC Sink is here](https://github.com/confluentinc/kafka-connect-jdbc/blob/master/src/main/java/io/confluent/connect/jdbc/dialect/PostgreSqlDatabaseDialect.java#L299).\\n:::\\n\\n### Case insensitive\\nBy default, the JDBC Sink will use quotes for all table and column names, which is usually fine, but PostgreSQL is case insensitive if quotes are not used, so if your data from Kafka comes with uppercase letters, for example, if you are using `camelCase`, but if you are not using quotes in your database, or you do not want to use them while querying, you should disable quotes in Kafka Connect with:\\n```\\nquote.sql.identifiers=never\\n```\\n\\n\\n\\n## Deploy new connectors in K8s\\nDeploying new connectors can be tricky, especially if you are using Kubernetes. Kafka Connect exposes an API that we can use to create new connectors, but we have to \\"manually\\" do some calls to create, update, or delete connectors. This is not the best way of integrating something on our CI/CD, especially if our CI is running outside our K8s cluster.\\n\\nIdeally, we would want to have a configuration file in our repository, that can be updated and automatically deployed during our CI.\\n\\n## connect-operator\\n\\nThere is a solution for this, Confluent\'s [connect-operator](https://github.com/confluentinc/streaming-ops/tree/main/images/connect-operator), although the solution is not very robust, it does the job.\\n\\nThis is based on the [shell-operator](https://github.com/flant/shell-operator), a Kubernetes operator that can be deployed in our cluster and configured to \\"listen\\" for specific events, like new deployments, changes in config maps or whatever we want.\\nSpecifically, this `connect-operator` is designed to listen for changes in a config map, and then it will create, update, or delete connectors based on the content of that config map.\\n\\nIn other words, we can put our Connector\'s configuration in a config map, and then the `connect-operator` will create the connector for us.\\n\\n:::tip\\nThe `connect-operator` does not need to be deployed together with Kafka Connect, it is an independent pod that will be running in our cluster, \\nit can be in the same namespace or not. It also can listen for config-maps attached to our Kafka Connect or to any other deployment, \\nall depending on our configuration and K8s permissions.\\n:::\\n\\n:::warning\\nThe `connect-operator` is a nice tool that does the job, but it isn\'t very robust. For example, it does not check if a connector creation has failed or not, it only checks if the connector exists or not, sends a `curl` to the REST API, and then it assumes that everything is fine.\\nIn any case, it is just a bash script using JQ for configuration, so it can be easily modified to fit our needs.\\n:::\\n\\n### Configuring the connect-operator\\nAs the `connect-operator` is based on the `shell-operator`, it expects a configuration file in YAML format, where we can define the events we want to listen to.\\n\\nBy default, the operator is called in two ways:\\n\\n- At startup, it will be called with a flag `--config` and it has to return the configuration file in YAML format that specifies the events we want to listen to.\\n- When an event is triggered, our script will be triggered with the event as a parameter.\\n\\n### Listening to config maps\\nThe config that we have to return to listen for config map changes should see something similar to this:\\n```yaml\\nconfigVersion: v1\\nkubernetes:\\n- name: ConnectConfigMapMonitor\\n  apiVersion: v1\\n  kind: ConfigMap\\n  executeHookOnEvent: [\\"Added\\",\\"Deleted\\",\\"Modified\\"]\\n  jqFilter: \\".data\\"\\n  labelSelector:\\n    matchLabels:\\n      destination: $YOUR_DESTINATION\\n  namespace:\\n    nameSelector:\\n      matchNames: [\\"$YOUR_NAMESPACE\\"]\\n```\\n`YOUR_DESTINATION` must be the same as the label used in the config map, and `YOUR_NAMESPACE` must be the same as the namespace where the config map is deployed.\\n\\n:::info\\nThe default `connect-operator` has a config to enable or disable the connector, but it is done in a way that will enable or disable all your connectors at once, so I prefer to skip that part as I want to have multiple connectors in my config maps.\\n:::\\n\\n:::note\\nThe configuration looks different from a standard K8s configuration, but the `shell-operator` can handle it and **there is no need to declare a new [CRD](https://helm.sh/docs/chart_best_practices/custom_resource_definitions/) with that structure.**\\n:::\\n\\n### Config Map\\nThe config map must have the connectors\' configuration in JSON, in the same way, you will use it in the REST API. \\nI would suggest building a Helm template for config maps, so you can write your connectors configuration in YAML and then convert it to JSON using Helm.\\n\\n\\nSomething like this should work in Helm:\\n```gotemplate\\n{{- if .Values.configMap }}\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  labels:\\n    destination: {{ .Values.your-deployment-name }} # this has to match with the label in the connect-operator config\\ndata:\\n  {{- range $key, $value := .Values.configMap.data }}\\n    {{ $key }}: {{ $value | toJson | quote | indent 6 | trim }}\\n  {{- end }}\\n{{- end }}\\n```\\n\\nAfter having this Helm template, we can write our Connector\'s config like this:\\n```yaml\\nconfigMap:\\n  data:\\n    my-connector-name:\\n      name: \\"my-connector-name\\"\\n      config:\\n        # JDBC Config\\n        name: \\"my-connector-name\\"\\n        connector.class: \\"io.confluent.connect.jdbc.JdbcSinkConnector\\"\\n        # using IAM Auth\\n        connection.url: \\"jdbc:aws-wrapper:postgresql://{host}:{port}/postgres?wrapperPlugins=iam&wrapperDialect=rds-pg\\"\\n        connection.user: env.USERNAME\\n        dialect.name: \\"PostgreSqlDatabaseDialect\\"\\n        topics: \\"my-topic\\"\\n        tasks.max: \\"4\\"\\n        # ...\\n```\\n\\n:::tip\\nThe config-map can be attached to your Kafka Connect deployment, or to any other deployment, what matters is that the `connect-operator` can find it.\\n:::\\n\\nOnce it is deployed as a config-map, the `connect-operator` will create the connector for us.\\n\\n### Replacing variables\\nThe `connect-operator` uses `jq` to replace variables, they can be stored in some config files inside the docker, passed as arguments, or as environment variables.\\nHaving them inside config files inside the Docker images looks weird to me, why our operator should know about the context of our connectors?\\n\\nThese are some examples of replacing variables, but you can find more details in the `jq` documentation:\\n\\nVariables:\\n```yaml\\nconnection.user: $username\\n```\\n\\nEnvironment variables:\\n```yaml\\nconnection.user: env.USERNAME\\n```\\nNote that they are not between quotes.\\n\\nVariables inside strings:\\n```yaml\\nconnection.user: \\"prefix_\\\\(env.USERNAME)_suffix\\"\\n```\\n:::tip\\nIf you are parsing this with Helm, you might need to have the string between single quotes, otherwise, Helm will fail on `\\\\(`\\n:::\\n\\n\\n\\n\\n### RBAC permissions to read config maps\\nIf your `connect-operator` stays in a different deployment than the config-map, you will need to give it permission to read the config map. This can be achieved using a Role and a RoleBinding using Helm.\\n\\nSomething like this needs to be created:\\n\\n```yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  name: {{ .Values.your-app }}-configmap-read-role\\n  namespace: {{ .Release.Namespace }}\\nrules:\\n  - apiGroups: [\\"\\"]\\n    resources: [\\"configmaps\\"]\\n    verbs: [\\"list\\", \\"watch\\"] # List and Watch all configmaps, get only the ones specified in resourceNames\\n  - apiGroups: [\\"\\"]\\n    resources: [\\"configmaps\\"]\\n    resourceNames:\\n      [\\"your-destination\\"] # this is the deployment having the config-map\\n    verbs: [\\"get\\", \\"watch\\", \\"list\\"]\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: {{ .Values.your-app }}-read-configmaps\\n  namespace: {{ .Release.Namespace }}\\nsubjects:\\n  - kind: ServiceAccount\\n    name: {{ .Values.your-app }}\\nroleRef:\\n  kind: Role\\n  name: {{ .Values.your-app }}-configmap-read-role\\n  apiGroup: rbac.authorization.k8s.io\\n{{- end }}\\n```\\n\\n\\n## Custom Kafka groups for your connectors\\n\\nBy default, Kafka Connect will create a group for each connector, and it will use the connector\'s name as the group name, with `connect-` as a prefix.\\nThis is not very flexible, as we might want to have our own group names. For example, if we are sharing K8s clusters with other teams, \\nwe might want to have our own group names to avoid conflicts. Or we could have our own naming convention with ACLs in Kafka.\\n\\nTo decide a group name, we have to change two configurations:\\n\\nFirst, we have to create an environment variable that allows us to override some configs, including the group name:\\n```bash\\nCONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY=All\\n```\\n:::tip\\nThis can be on your deployment file or on your Dockerfile.\\n:::\\n\\nThen, we can add the group name to our connector\'s configuration:\\n```yaml\\nconsumer.override.group.id: \\"my-custom-group-name\\"\\n```"},{"id":"big-data-types-library","metadata":{"permalink":"/blog/big-data-types-library","editUrl":"https://github.com/JavierMonton/blog/edit/main/website/blog/2021-10-10-big-data-types/index.md","source":"@site/blog/2021-10-10-big-data-types/index.md","title":"Big Data Types Library","description":"Big Data Types is a library that can safely convert types between different Big Data systems.","date":"2021-10-10T00:00:00.000Z","formattedDate":"October 10, 2021","tags":[{"label":"Scala","permalink":"/blog/tags/scala"},{"label":"Spark","permalink":"/blog/tags/spark"},{"label":"Big-query","permalink":"/blog/tags/big-query"},{"label":"Cassandra","permalink":"/blog/tags/cassandra"},{"label":"Circe","permalink":"/blog/tags/circe"},{"label":"type-class","permalink":"/blog/tags/type-class"},{"label":"type-safe","permalink":"/blog/tags/type-safe"},{"label":"type-derivation","permalink":"/blog/tags/type-derivation"},{"label":"type-level-programming","permalink":"/blog/tags/type-level-programming"}],"readingTime":8.615,"hasTruncateMarker":false,"authors":[{"name":"Javier Mont\xf3n","title":"Software Engineer","url":"https://github.com/JavierMonton","imageURL":"https://github.com/JavierMonton.png","key":"javier"}],"frontMatter":{"slug":"big-data-types-library","title":"Big Data Types Library","authors":"javier","tags":["Scala","Spark","Big-query","Cassandra","Circe","type-class","type-safe","type-derivation","type-level-programming"]},"unlisted":false,"prevItem":{"title":"Kafka Connect - JDBC Sink","permalink":"/blog/kafka-connect-jdbc-sink-iam-auth"}},"content":"import Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n\\nBig Data Types is a library that can safely convert types between different Big Data systems.\\n\\n## The power of the library\\n\\nThe library implements a few abstract types that can hold any kind of structure, and using type-class derivations,\\nit can convert between multiple types without having any code relating them. In other words, there is no need to implement a transformation between type A to type B, the library will do it for you.\\n\\nAs an example, let\'s say we have a generic type called `Generic`. Now we want to convert from type `A` to type `B`. \\nIf we implement the conversion from `A` to `Generic` and the conversion from `Generic` to `B`, automatically we can convert from `A` to `B` although there is no single line of code mixing `A` and `B`.\\n\\n\\nWe can also do the opposite, we can convert from `B` to `A` by implementing the conversion from `B` to `Generic` and the conversion from `Generic` to `A`. Now we can convert between `A` and `B` as we wish.\\n\\n\\nNow comes the good part of this. If we introduce a new type `C` and we want to have conversions, we would need to convert from `A` to `C`, and from `B` to `C` and the opposite **(4 new implementations)**. \\nIf now we introduce `D`, we would need to implement the conversion from `A` to `D`, from `B` to `D` and from `C` to `D` and the opposite **(6 new implementations)**. This is not scalable, and it is not maintainable. \\n\\n\\nHaving this `Generic` type means that when we introduce `C`, we only need to implement the conversion from `C` to `Generic` and from `Generic` to `C`, without worrying at all about other implementations or types. \\nMoreover, is likely that the conversion will be very similar to others, so we can reuse some of the code.\\n\\n:::tip\\nIt is important to know that one of these types is the Scala types themselves. So if we want to convert from `Scala types` (like `case classes`) to another type, we only need to implement `Generic -> newType`\\n:::\\n\\n## How the library works\\n\\n\\n### Modules\\nAs mentioned, the library has multiple modules, each one of them representing a different system with its own types. Each module implements the conversion from and to `Generic`.\\n\\nFor now, the modules are `core` (for Scala types and common code), `BigQuery`, `Cassandra`, `Circe`, and `Spark`.\\n\\nTo use the library, only the modules that are needed should be imported. For example, if we want to convert from `Scala types` to `BigQuery` types, we only need to `BigQuery` module. (`Core` module is always included as a dependency)\\nIf we want to convert from `Spark` to `BigQuery` we need to import both `Spark` and `BigQuery` modules. \\n\\n### Generic type\\n\\nThe `Generic` type is called `SqlType` and it\'s implemented as [sealed trait](https://github.com/data-tools/big-data-types/blob/main/core/src/main/scala_2/org/datatools/bigdatatypes/basictypes/SqlType.scala) that can hold any kind of structure. \\nIn Scala 3, this type is implemented as an [enum](https://github.com/data-tools/big-data-types/blob/main/core/src/main/scala_3/org/datatools/bigdatatypes/basictypes/SqlType.scala) but both represents the same.\\n\\n#### Repeated values\\nUsually, there are two ways of implementing a repeated value like an Array. Some systems use a type like `Array` or `List` \\nand others flag a basic type with `repeated`. The implementation of this `SqlType` uses the latter, \\nso any basic type can have a `mode` that can be `Required`, `Nullable`, or `Repeated`. This is closer to the `BigQuery` implementation.\\n\\n:::note\\nThis implementation does not allow for `Nullable` and `Repeated` at the same time, but a `Repeated` type can have 0 elements.\\n:::\\n\\n#### Nested values\\nThe `SqlStruct` can hold a list of records, including other `SqlStruct`, meaning that we can have nested structures.\\n\\n## Type-class derivation\\n\\nType-classes are a way of implementing \\"ad-hoc polymorphism\\". This means that we can implement behaviour for a type without having to modify the type itself.\\nIn Scala, we achieve this through implicits.\\n\\nThe interesting part of type-classes for this library is that we can derive a type-class for a type without having to implement it.\\n\\nFor example, we can create a simple type-class:\\n\\n```scala\\ntrait MyTypeClass[A] {\\n  def doSomething(a: A): String\\n}\\n```\\n:::tip\\nA type-class is always a `trait` with a generic type.\\n:::\\nThen, we can implement our type-class for an `Int` type:\\n\\n```scala\\nimplicit val myTypeClassForInt: MyTypeClass[Int] = new MyTypeClass[Int] {\\n  override def doSomething(a: Int): String = \\"This is my int\\" + a.toString\\n}\\n```\\n:::tip\\nScala 2.13 has a simplified syntax for this when there is only one method in the trait:\\n```scala\\nimplicit val myTypeClassForInt: MyTypeClass[Int] = (a: Int) => \\"This is my int\\" + a.toString\\n```\\n:::\\nWe can do similar for other types:\\n\\n```scala\\nimplicit val myTypeClassForString: MyTypeClass[String] = new MyTypeClass[String] {\\n  override def doSomething(a: String): String = \\"This is my String\\" + a\\n}\\n```\\n\\nNow, if we want to have a `List[Int]` or a `List[String]`, and use our type-class, we need to implement both `List[Int]` and `List[String]`.\\n**But**, if we implement the type-class for `List[A]` where `A` is any type, the compiler can derive the implementation for `List[Int]` and `List[String]` automatically, and for any other type already implemented.\\n\\n```scala\\nimplicit def myTypeClassForList[A](implicit myTypeClassForA: MyTypeClass[A]): MyTypeClass[List[A]] = new MyTypeClass[List[A]] {\\n  override def doSomething(a: List[A]): String = a.map(myTypeClassForA.doSomething).mkString(\\",\\")\\n}\\n```\\n\\nSimilarly, if we want to have a `case class` like:\\n```scala\\ncase class MyClass(a: Int, b: String)\\n```\\nWe would need to implement the type-class for `MyClass`. But, if we implement the type-class for a generic `Product` type, the compiler can derive the implementation for `MyClass` automatically, and for any other `case class` that has types already implemented.\\n\\n:::note\\nImplementing the conversion for a `Product` type is more complex than implementing it for a `List` type, and usually [Shapeless](https://github.com/milessabin/shapeless) is the library we use to do this in Scala 2.\\n\\nIn Scala 3, the language already allows us to derive the type-class for a `Product` type, so we don\'t need to use Shapeless.\\n\\nIn [big-data-types](https://github.com/data-tools/big-data-types) we have the implementation for all basic types, including iterables and `Product` types [here for Scala 2](https://github.com/data-tools/big-data-types/blob/main/core/src/main/scala_2/org/datatools/bigdatatypes/conversions/SqlTypeConversion.scala) \\nand [here for Scala 3](https://github.com/data-tools/big-data-types/blob/main/core/src/main/scala_3/org/datatools/bigdatatypes/conversions/SqlTypeConversion.scala).\\n:::\\n\\n## Implementing a new type\\n\\nTo implement a new type, we need to implement the conversion from and to `Generic` type. There is a complete guide, step by step, with examples, in the [official documentation](https://data-tools.github.io/big-data-types/docs/Contributing/CreateNewType)\\n\\n\\nA quick example, let\'s say we want to implement a new type called `MyType`. We need to implement the conversion `MyType -> Generic` and `Generic -> MyType`.\\n:::tip\\nBoth conversions are not strictly needed, if we only need to use `Scala -> MyType` we only need to implement `Generic -> MyType`\\nbecause the library already has the conversion `Scala -> Generic`. The same happens with other types, like `BigQuery -> MyType` will also be ready automatically.\\n:::\\n\\nTo do that, we need a type-class that works with our type. This will be different depending on the type we want to implement.\\nFor example:\\n```scala\\ntrait GenericToMyType[A] {\\n  def getType: MyTypeObject\\n}\\n```\\nMaybe our type works with a List at the top level, as Spark does, so instead, we will do:\\n```scala\\ntrait GenericToMyType[A] {\\n  def getType: List[MyTypeObject]\\n}\\n```\\n:::tip\\n`getType` can be renamed to anything meaningful, like `toMyType` or `myTypeSchema`\\n:::\\n\\nAnd we need to implement this type-class for all the (Generic) `SqlType` types:\\n<Tabs>\\n    <TabItem value=\\"Scala2\\" label=\\"Scala 2\\" default>\\n        ```scala\\n        implicit val genericToMyTypeForInt: GenericToMyType[SqlInt] = new GenericToMyType[SqlInt] {\\n          override def getType: MyTypeObject = MyIntType\\n        }\\n        ```\\n    </TabItem>\\n    <TabItem value=\\"Scala3\\" label=\\"Scala 3\\" default>\\n        ```scala\\n        given GenericToMyType[SqlInt] = new GenericToMyType[SqlInt] {\\n            override def getType: MyTypeObject = MyIntType\\n        }\\n        ```\\n    </TabItem>\\n</Tabs>\\n\\n## Using conversions\\n\\nThe defined type-classes allow you to convert `MyType -> Generic` by doing this:\\n```scala\\nval int: SqlInt = SqlTypeConversion[MyIntType].getType\\n```\\nAnd `Generic -> MyType` by doing this:\\n```scala\\nval int: MyIntType = SqlTypeToBigQuery[SqlInt].getType\\n```\\n\\nThis can work well when we work these `case classes` and we don\'t have an instance of them. For example, a `case class` definition can be converted into a `BigQuery` Schema, ready to be used for table creation.\\n\\nBut, sometimes, our types work with instances rather than definitions, and we need to use them to convert to other types. \\n\\nThere is another type-class on all implemented types that allows to work with instances. In general, this type-class can be implemented using code from the other, but this one expects an argument of the type we want to convert to.\\n```scala\\ntrait SqlInstanceToMyType[A] {\\n  def myTypeSchema(value: A): MyTypeObject\\n}\\n```\\n\\nImplementing this type-class allows to use the conversion like this:\\n```scala\\nval mySchema: MyTypeObject = SqlInstanceToMyType.myTypeSchema(theOtherType)\\n```\\n\\nBut these syntaxis are not very friendly, and we can use extension methods to make it more readable.\\n\\n## Extension methods\\nExtension methods in Scala 2 are done through implicit classes and allow us to create new methods for existing types.\\n\\nIn the library, we implement extension methods for `Generic -> SpecificType`, and the interesting part, again, is that we don\'t need to implement `A -> B` directly, the compiler can derive it for us.\\n\\n<Tabs>\\n    <TabItem value=\\"Scala2\\" label=\\"Scala 2\\" default>\\n        ```scala\\n          implicit class InstanceSyntax[A: SqlInstanceToMyType](value: A) {\\n            def asMyType: MyTypeObject = SqlInstanceToMyType[A].myTypeSchema(value)\\n          }\\n        ```\\n    </TabItem>\\n    <TabItem value=\\"Scala3\\" label=\\"Scala 3\\" default>\\n        ```scala\\n          extension[A: SqlInstanceToMyType](value: A) {\\n            def asMyType: MyTypeObject = SqlInstanceToMyType[A].myTypeSchema(value)\\n          }\\n        ```\\n    </TabItem>\\n</Tabs>\\n\\n\\nand suddenly, we can use the conversion like this:\\n```scala\\nval mySchema: MyTypeObject = theOtherType.asMyType\\n```\\n\\nAnd this is a syntax that can be easier to use. For example, if we work with Spark and BigQuery, we can do the following:\\n```scala\\nval sparkDf: DataFrame = ???\\nval bigQuerySchema = sparkDf.schema.asBigQuery\\n```\\n\\n\\n## More types to come\\n\\nThe library has only a few types implemented (BigQuery, Spark, Cassandra, and Circe) but implementing a new type is fairly easy and it gets automatically methods that can be used to convert it into any other type already implemented. \\nAs this grows, the number of conversions grows exponentially, and the library becomes more powerful.\\n\\nSome types that could be potentially implemented:\\n- Avro\\n- Parquet\\n- Athena (AWS)\\n- Redshift (AWS)\\n- Snowflake\\n- RDS (relational databases)\\n- Protobuf\\n- ElasticSearch templates\\n- ...\\n\\nSome types could have some restrictions, but they could be implemented differently, for example, \\na type conversion could be implemented as a `String` conversion, being the string a \\"Create table\\" statement for a specific database \\nand automatically any other type could be printed as a \\"Create table\\" statement."}]}')}}]);